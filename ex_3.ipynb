{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3a: TIMESERIES PATTERN TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.680044</td>\n",
       "      <td>-0.152540</td>\n",
       "      <td>0.463497</td>\n",
       "      <td>1.125701</td>\n",
       "      <td>1.784473</td>\n",
       "      <td>2.387815</td>\n",
       "      <td>2.886590</td>\n",
       "      <td>3.239594</td>\n",
       "      <td>3.417875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.669457</td>\n",
       "      <td>-0.669649</td>\n",
       "      <td>-0.667227</td>\n",
       "      <td>-0.663867</td>\n",
       "      <td>-0.659006</td>\n",
       "      <td>-0.653538</td>\n",
       "      <td>-0.648224</td>\n",
       "      <td>-0.643643</td>\n",
       "      <td>-0.640175</td>\n",
       "      <td>-0.637993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.232027</td>\n",
       "      <td>2.863343</td>\n",
       "      <td>3.229689</td>\n",
       "      <td>3.284083</td>\n",
       "      <td>3.025244</td>\n",
       "      <td>2.488196</td>\n",
       "      <td>1.742625</td>\n",
       "      <td>0.890124</td>\n",
       "      <td>0.062662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.697279</td>\n",
       "      <td>-0.702275</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-0.714786</td>\n",
       "      <td>-0.722466</td>\n",
       "      <td>-0.730555</td>\n",
       "      <td>-0.737959</td>\n",
       "      <td>-0.743156</td>\n",
       "      <td>-0.744424</td>\n",
       "      <td>-0.741200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2.746582</td>\n",
       "      <td>2.727073</td>\n",
       "      <td>2.688213</td>\n",
       "      <td>2.630574</td>\n",
       "      <td>2.554928</td>\n",
       "      <td>2.462240</td>\n",
       "      <td>2.353654</td>\n",
       "      <td>2.230476</td>\n",
       "      <td>2.094162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474689</td>\n",
       "      <td>-0.403397</td>\n",
       "      <td>-0.331573</td>\n",
       "      <td>-0.260273</td>\n",
       "      <td>-0.190542</td>\n",
       "      <td>-0.123397</td>\n",
       "      <td>-0.059818</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>0.053012</td>\n",
       "      <td>0.100628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7   \\\n",
       "0  10 -0.680044 -0.152540  0.463497  1.125701  1.784473  2.387815  2.886590   \n",
       "1   4  2.232027  2.863343  3.229689  3.284083  3.025244  2.488196  1.742625   \n",
       "2  10  2.746582  2.727073  2.688213  2.630574  2.554928  2.462240  2.353654   \n",
       "\n",
       "         8         9   ...        90        91        92        93        94  \\\n",
       "0  3.239594  3.417875  ... -0.669457 -0.669649 -0.667227 -0.663867 -0.659006   \n",
       "1  0.890124  0.062662  ... -0.697279 -0.702275 -0.707983 -0.714786 -0.722466   \n",
       "2  2.230476  2.094162  ... -0.474689 -0.403397 -0.331573 -0.260273 -0.190542   \n",
       "\n",
       "         95        96        97        98        99  \n",
       "0 -0.653538 -0.648224 -0.643643 -0.640175 -0.637993  \n",
       "1 -0.730555 -0.737959 -0.743156 -0.744424 -0.741200  \n",
       "2 -0.123397 -0.059818 -0.000729  0.053012  0.100628  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "## load the datasets\n",
    "dataset_path = \"/Users/sebastianodarconso/Desktop/università/magistrale_progetti/bdss/UCRArchive_2018/MedicalImages/MedicalImages_TRAIN.tsv\"\n",
    "dataset = pd.read_csv(dataset_path, sep=\"\\t\", header=None)\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class Node \n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.rule = None\n",
    "        self.association_rules = None\n",
    "        self.class_label = None\n",
    "        self.information_gain = 0.0 \n",
    "        self.true_branch = None\n",
    "        self.false_branch = None\n",
    "        self.dataset_length = 0 \n",
    "        self.ts = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support(dataset, pattern):\n",
    "    dataset_list = dataset.iloc[:, 1:].values.tolist()\n",
    "    count = 0\n",
    "    for transaction in dataset_list:\n",
    "        if pattern_matched(pattern, transaction):\n",
    "            count += 1\n",
    "    return count / len(dataset)\n",
    "\n",
    "\n",
    "def pattern_matched(sequence, transaction):\n",
    "    for value, is_greater, index, _ in sequence:\n",
    "        if index >= len(transaction):\n",
    "            return False\n",
    "        if is_greater:\n",
    "            if not transaction[index] > value:\n",
    "                return False\n",
    "        else:\n",
    "            if not transaction[index] < value:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_association_rules(dataset, pts, confidence_threshold):\n",
    "#     highest_confidence_rule = None\n",
    "#     max_confidence = -1\n",
    "#     best_split_index = -1\n",
    "    \n",
    "#     sequence_support = calculate_support(dataset, pts)\n",
    "\n",
    "#     for i in range(1, len(pts)):\n",
    "#         antecedent = pts[:i]\n",
    "#         antecedent_support = calculate_support(dataset, antecedent)\n",
    "#         confidence = sequence_support / antecedent_support if antecedent_support > 0 else 0\n",
    "        \n",
    "#         if confidence >= confidence_threshold and confidence > max_confidence:\n",
    "#             max_confidence = confidence\n",
    "#             highest_confidence_rule = (pts, confidence, i)\n",
    "#             best_split_index = i\n",
    "            \n",
    "#     return highest_confidence_rule if highest_confidence_rule else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Association rules extraction\n",
    "The previous function gives the association rules that have a greater or equal confidence than the threshold, this usually results in association rules that have only one consequent, making it less \"informative\".\n",
    "Now the new function returns the association rule before the one that gives a greater or equal confidence than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_association_rules(dataset, pts, confidence_threshold):\n",
    "    sequence_support = calculate_support(dataset, pts)\n",
    "    best_pts = None\n",
    "    for i in range(1, len(pts)):\n",
    "        antecedent = pts[:i]\n",
    "        antecedent_support = calculate_support(dataset, antecedent)\n",
    "        confidence = sequence_support / antecedent_support if antecedent_support > 0 else 0\n",
    "\n",
    "        if confidence >= confidence_threshold:\n",
    "            break\n",
    "        \n",
    "        best_pts = (pts, confidence, i)\n",
    "    \n",
    "    return best_pts if best_pts else None # Return None if no such index is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidates generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## time series sample function\n",
    "def random_ts_sample(dataset, indexes):\n",
    "    sampled = dataset.sample()\n",
    "    while sampled.index[0] in indexes:\n",
    "        sampled = dataset.sample()\n",
    "    \n",
    "    return sampled, sampled.index[0] + 1\n",
    "\n",
    "## generate candidates function \n",
    "def generate_candidates(dataset, min_support):\n",
    "\n",
    "    ## initialize list of indexes \n",
    "    I = list()\n",
    "\n",
    "    ## initialize list of candidates \n",
    "    candidates = list()\n",
    "\n",
    "    ## initialize list of patterns \n",
    "    pts = list()\n",
    "\n",
    "    while len(pts) <= 2 or calculate_support(dataset, pts) >= min_support:\n",
    "\n",
    "        if (len(pts) >= len(dataset.columns) - 1):\n",
    "            break\n",
    "    \n",
    "        ## sample random time series\n",
    "        random_ts, ts_index = random_ts_sample(dataset, I)\n",
    "        random_ts_class = random_ts[0].values[0]\n",
    "        ts_as_list = random_ts.values.tolist()[0][1:]\n",
    "\n",
    "        ## sample random index and add it to I\n",
    "        random_index = random.randint(0, len(ts_as_list) - 1)\n",
    "        while random_index in I:\n",
    "            random_index = random.randint(0, len(ts_as_list) - 1)\n",
    "        I.append(random_index)\n",
    "\n",
    "        ## sample random time series prime \n",
    "        random_ts_prime, ts_prime_index = random_ts_sample(dataset, I)\n",
    "        while ts_prime_index == ts_index:\n",
    "            random_ts_prime, ts_prime_index = random_ts_sample(dataset, I)\n",
    "        \n",
    "        random_ts_prime_as_list = random_ts_prime.values.tolist()[0][1:]    \n",
    "\n",
    "        candidate = (ts_as_list[random_index],\n",
    "                    random_ts_prime_as_list[random_index] >= ts_as_list[random_index],\n",
    "                    random_index, \n",
    "                    random_ts_class)\n",
    "\n",
    "        candidates.append(candidate)\n",
    "        pts = sorted(candidates, key=lambda x: x[2])\n",
    "\n",
    "\n",
    "    ## remove last element because it does not satisfy the min_support condition\n",
    "    return pts[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    total_count = len(labels)\n",
    "    entropy = 0.0\n",
    "    \n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_count\n",
    "        if probability > 0:\n",
    "            entropy -= probability * math.log2(probability)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(parent_labels, true_labels, false_labels):\n",
    "    parent_entropy = calculate_entropy(parent_labels)\n",
    "    true_entropy = calculate_entropy(true_labels)\n",
    "    false_entropy = calculate_entropy(false_labels)\n",
    "    \n",
    "    total_count = len(parent_labels)\n",
    "    true_weight = len(true_labels) / total_count\n",
    "    false_weight = len(false_labels) / total_count\n",
    "    \n",
    "    info_gain = parent_entropy - (true_weight * true_entropy + false_weight * false_entropy)\n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split functions \n",
    "Branches that satisfy the rules and branches that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_rule(ts, rule):\n",
    "    tsv_true = []\n",
    "    tsv_false = []\n",
    "    \n",
    "    for (value, condition, index, _) in rule:\n",
    "        if index >= len(ts):\n",
    "            return False\n",
    "        if condition == True and ts[index] < value:\n",
    "            tsv_false.append((ts[index], index))\n",
    "            return False\n",
    "        elif condition == False and ts[index] > value:\n",
    "            tsv_false.append((ts[index], index))\n",
    "            return True\n",
    "        else:\n",
    "            tsv_true.append((ts[index], index))\n",
    "        \n",
    "    return True\n",
    "\n",
    "def dataset_split(dataset, rules):\n",
    "    dataset_list = dataset.iloc[:, :].values.tolist()\n",
    "    false_subset = []\n",
    "    true_subset = []\n",
    "\n",
    "    for ts in dataset_list:\n",
    "        if matches_rule(ts[1:], rules):\n",
    "            true_subset.append(ts)\n",
    "        else:\n",
    "            false_subset.append(ts)\n",
    "    \n",
    "    return false_subset, true_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rpts_tree(Ts, labels, gain=-1, max_height=float('inf'), MIN_SUPPORT=None, min_samples=1, depth=0):\n",
    "\n",
    "#     best_info_gain = gain\n",
    "#     min_gain = 0.01\n",
    "#     CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "#     if depth >= max_height or len(Ts) < min_samples:\n",
    "#         return None\n",
    "    \n",
    "#     ## create a new node\n",
    "#     v = Node()\n",
    "#     v.ts = Ts\n",
    "\n",
    "#     ## get the most common label\n",
    "#     v.class_label = Counter(labels).most_common(1)[0][0]\n",
    "#     if not any(label == v.class_label for label in labels):\n",
    "#         return v   \n",
    "\n",
    "#     ## generate candidates\n",
    "#     pts = generate_candidates(dataset, MIN_SUPPORT)\n",
    "\n",
    "#     association_rules = extract_association_rules(dataset, pts, CONFIDENCE_THRESHOLD)\n",
    "\n",
    "#     if association_rules is not None:\n",
    "#         pattern, confidence, split_index = association_rules\n",
    "#         ## add association rule to node\n",
    "#         v.association_rules = \"Antecedent:\" + str(pattern[:split_index]) + \"  ==> Consequent:\" + str(pattern[split_index:]) + \" Confidence:\" + str(round(confidence, 2)) + \" Split Index:\" + str(split_index)\n",
    "#         final_candidates = pattern[:split_index] + pattern[split_index:]\n",
    "#     else:\n",
    "#         final_candidates = pts\n",
    "\n",
    "#     if not final_candidates:\n",
    "#         return v  \n",
    "    \n",
    "#     v.rule = final_candidates\n",
    "#     true_subset, false_subset = dataset_split(Ts, final_candidates)\n",
    "\n",
    "#     if not true_subset or not false_subset:\n",
    "#         return v \n",
    "\n",
    "#     labels_true = [ts[0] for ts in true_subset]\n",
    "#     labels_false = [ts[0] for ts in false_subset]\n",
    "#     ig = calculate_information_gain(labels, labels_true, labels_false)\n",
    "\n",
    "#     if ig < min_gain:\n",
    "#         return v\n",
    "    \n",
    "#     if ig > best_info_gain:\n",
    "#         best_info_gain = ig\n",
    "#     else:\n",
    "#         return v\n",
    "    \n",
    "#     v.information_gain = calculate_information_gain(labels, labels_true, labels_false)\n",
    "\n",
    "#     print(f\"Depth: {depth}, Rule: {v.rule if v.association_rules is None else v.association_rules}, Information Gain: {v.information_gain:.4f}, Class Label: {v.class_label}\")\n",
    "#     v.dataset_length = len(Ts)\n",
    "#     v.true_branch = rpts_tree(pd.DataFrame(true_subset), labels_true, best_info_gain, max_height, MIN_SUPPORT, min_samples, depth + 1)\n",
    "#     v.false_branch = rpts_tree(pd.DataFrame(false_subset), labels_false, best_info_gain, max_height, MIN_SUPPORT, min_samples, depth + 1)\n",
    "\n",
    "#     return v\n",
    "\n",
    "# # labels = dataset[0].values.tolist()\n",
    "# # rpts_tree(dataset, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New tree generation\n",
    "The previous rpts tree function generates only one pattern per node, resulting in trees that might perform bad in terms of information gain, this version generates **n** candidates, where **n** is user-inserted, then for each candidate takes only the one that has the best information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def rpts_tree(Ts, labels, n_candidates, max_height=float('inf'), MIN_SUPPORT=None, min_samples=1, depth=0):\n",
    "    min_gain = 0.01\n",
    "    CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "    if depth >= max_height or len(Ts) < min_samples:\n",
    "        return None\n",
    "    \n",
    "    ## create a new node\n",
    "    v = Node()\n",
    "    v.ts = Ts\n",
    "\n",
    "    ## get the most common label\n",
    "    v.class_label = Counter(labels).most_common(1)[0][0]\n",
    "    if not any(label == v.class_label for label in labels):\n",
    "        return v   \n",
    "\n",
    "    ## generate candidates\n",
    "    all_candidates = []\n",
    "    for _ in range(n_candidates):\n",
    "        pts = generate_candidates(dataset, MIN_SUPPORT)\n",
    "        all_candidates.append(pts)\n",
    "\n",
    "    best_candidate = None\n",
    "    best_info_gain = -1\n",
    "    best_true_subset = None\n",
    "    best_false_subset = None\n",
    "\n",
    "    for pts in all_candidates:\n",
    "\n",
    "        true_subset, false_subset = dataset_split(Ts, pts)\n",
    "        if not true_subset or not false_subset:\n",
    "            continue\n",
    "\n",
    "        labels_true = [ts[0] for ts in true_subset]\n",
    "        labels_false = [ts[0] for ts in false_subset]\n",
    "\n",
    "        ig = calculate_information_gain(labels, labels_true, labels_false)\n",
    "\n",
    "        if ig > best_info_gain:\n",
    "            best_info_gain = ig\n",
    "            best_candidate = pts\n",
    "            best_true_subset = true_subset\n",
    "            best_false_subset = false_subset\n",
    "\n",
    "    if best_candidate is None or best_info_gain < min_gain:\n",
    "        return v\n",
    "    \n",
    "    v.rule = best_candidate\n",
    "    v.information_gain = best_info_gain\n",
    "    v.dataset_length = len(Ts)\n",
    "\n",
    "    association_rules = extract_association_rules(dataset, best_candidate, CONFIDENCE_THRESHOLD)\n",
    "\n",
    "    if association_rules is not None:\n",
    "        pattern, confidence, split_index = association_rules\n",
    "        ## add association rule to node\n",
    "        v.association_rules = \"Antecedent:\" + str(pattern[:split_index]) + \"  ==> Consequent:\" + str(pattern[split_index:]) + \" Confidence:\" + str(round(confidence, 2)) + \" Split Index:\" + str(split_index) \n",
    "        final_candidates = pattern[:split_index] + pattern[split_index:]\n",
    "    else:\n",
    "        final_candidates = pts\n",
    "\n",
    "    if not final_candidates:\n",
    "        return v\n",
    "    \n",
    "    v.rule = final_candidates\n",
    "    \n",
    "    print(f\"Depth: {depth}, Rule: {v.rule if v.association_rules is None else  v.association_rules}, Information Gain: {v.information_gain:.4f}, Class Label: {v.class_label}\")\n",
    "    v.dataset_length = len(Ts)\n",
    "    v.true_branch = rpts_tree(pd.DataFrame(best_true_subset), labels_true, n_candidates, max_height, MIN_SUPPORT, min_samples, depth + 1)\n",
    "    v.false_branch = rpts_tree(pd.DataFrame(best_false_subset), labels_false, n_candidates,  max_height, MIN_SUPPORT, min_samples, depth + 1)\n",
    "\n",
    "    return v\n",
    "\n",
    "# labels = dataset[0].values.tolist()\n",
    "# rpts_tree(dataset, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    if node is None:\n",
    "        return\n",
    "\n",
    "    # Check and skip nodes where information gain is 0\n",
    "    if hasattr(node, 'information_gain') and node.information_gain == 0.0:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nDepth: \", depth)\n",
    "    indent = \"  \" * depth\n",
    "    print(f\"{indent}Node class: {node.class_label}\")\n",
    "    print(f\"{indent}Dataset Length: {node.dataset_length}\")\n",
    "\n",
    "    if hasattr(node, 'rule') and node.rule:\n",
    "        print(f\"{indent}Rule Length: {len(node.rule)}\")\n",
    "\n",
    "    if hasattr(node, 'information_gain'):\n",
    "        # Check if information_gain is not None before printing\n",
    "        if node.information_gain is not None:\n",
    "            print(f\"{indent}Information Gain: {node.information_gain:.4f}\")\n",
    "        else:\n",
    "            print(f\"{indent}Information Gain: None\")\n",
    "    \n",
    "    if node.true_branch or node.false_branch:\n",
    "        # Print true branch if it exists and has significant information gain\n",
    "        if node.true_branch and (not hasattr(node.true_branch, 'information_gain') or node.true_branch.information_gain != 0.0):\n",
    "            print(f\"{indent}True Branch:\")\n",
    "            print_tree(node.true_branch, depth + 1)\n",
    "        \n",
    "        # Print false branch if it exists and has significant information gain\n",
    "        if node.false_branch and (not hasattr(node.false_branch, 'information_gain') or node.false_branch.information_gain != 0.0):\n",
    "            print(f\"{indent}False Branch:\")\n",
    "            print_tree(node.false_branch, depth + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def print_ts(df, l, depth, branch):\n",
    "    # Get unique classes from the DataFrame\n",
    "    classes = df.iloc[:, 0].unique()\n",
    "    \n",
    "    # Define a color palette with enough colors for all unique classes\n",
    "    colors = px.colors.qualitative.Plotly[:len(classes)]  # Using Plotly's default qualitative color palette\n",
    "    if len(classes) > len(colors):\n",
    "        colors = px.colors.qualitative.Plotly * (len(classes) // len(px.colors.qualitative.Plotly) + 1)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    added_classes = []  # To track which classes have been added to the plot for legend purposes\n",
    "    \n",
    "    for idx in l:\n",
    "        ts = df.iloc[idx, 1:]  # Assuming the first column is the class and we skip it for the data\n",
    "        classe = df.iloc[idx, 0]  # Class is in the first column\n",
    "        \n",
    "        # Define the legend group based on class\n",
    "        legend_group = f'class_{classe}'\n",
    "        \n",
    "        # Determine whether to show in legend\n",
    "        show_in_legend = classe not in added_classes\n",
    "        \n",
    "        # Assign color based on class\n",
    "        color_idx = list(classes).index(classe)\n",
    "        color = colors[color_idx]\n",
    "        \n",
    "        # Add trace to the figure\n",
    "        fig.add_trace(go.Scatter(y=ts, mode='lines', \n",
    "                                 line=dict(color=color), \n",
    "                                 name=f'Class {classe}',\n",
    "                                 legendgroup=legend_group,\n",
    "                                 showlegend=show_in_legend))\n",
    "        \n",
    "        # Remember that this class has been added\n",
    "        if show_in_legend:\n",
    "            added_classes.append(classe)\n",
    "    \n",
    "    fig.update_layout(title='Visualizzazione Time-Series',\n",
    "                      xaxis_title='Tempo', \n",
    "                      yaxis_title='Ampiezza', \n",
    "                      legend_title='Class')\n",
    "    \n",
    "    # Save the figure as an HTML file\n",
    "    fig.write_html(f\"timeseries_output/timeseries_node{depth}_{branch}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree generation and visualization\n",
    "This might take some time due to the generation of html files (by default this option is disabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 0, Rule: Antecedent:[(0.15829449, True, 8, 5), (0.44479883, False, 11, 10), (0.40805209, False, 24, 2), (0.57644027, False, 37, 2), (0.51936704, False, 39, 3)]  ==> Consequent:[(-0.40281828, False, 66, 10), (-0.34465538, False, 67, 10)] Confidence:0.24 Split Index:5, Information Gain: 0.2095, Class Label: 10\n",
      "Depth: 1, Rule: Antecedent:[(-0.55186974, True, 0, 10), (-0.331507, False, 35, 10), (0.17604474, False, 48, 10)]  ==> Consequent:[(-0.55623409, False, 62, 10)] Confidence:0.16 Split Index:3, Information Gain: 1.6290, Class Label: 10.0\n",
      "Depth: 2, Rule: Antecedent:[(0.24385655, True, 1, 9), (1.0369738, False, 12, 10), (1.7587607, False, 16, 10), (-0.9657366, True, 17, 10), (-0.59259904, True, 28, 9), (0.024065439, False, 31, 10), (-0.57488495, True, 55, 10), (-0.1209455, False, 66, 5), (-0.4737542, False, 79, 10)]  ==> Consequent:[(-0.58373997, False, 92, 2)] Confidence:0.68 Split Index:9, Information Gain: 0.4445, Class Label: 10.0\n",
      "Depth: 3, Rule: Antecedent:[(1.3203494, True, 4, 10), (-0.51415042, True, 34, 9), (0.48346337, False, 36, 7), (0.12455856, False, 56, 10)]  ==> Consequent:[(-0.64650799, False, 70, 10)] Confidence:0.09 Split Index:4, Information Gain: 0.9852, Class Label: 7.0\n",
      "Depth: 3, Rule: Antecedent:[(1.0956624, True, 4, 10), (-0.27034504, True, 14, 10), (0.029278837, False, 15, 10), (-0.2019041, True, 16, 10)]  ==> Consequent:[(-0.2331365, True, 39, 9), (-0.1613679, True, 40, 9), (0.91368232, False, 58, 6)] Confidence:0.41 Split Index:4, Information Gain: 1.2495, Class Label: 7.0\n",
      "Depth: 4, Rule: Antecedent:[(0.09158915, True, 8, 4), (1.3388505, False, 53, 10)]  ==> Consequent:[(-0.42535103, False, 62, 2), (-1.2758665, True, 74, 10)] Confidence:0.38 Split Index:2, Information Gain: 0.9864, Class Label: 7.0\n",
      "Depth: 4, Rule: Antecedent:[(1.3733926, True, 6, 10), (0.11796801, False, 15, 10), (1.4444968, False, 30, 3), (-0.51883938, True, 59, 1), (-0.84601976, True, 80, 10)]  ==> Consequent:[(-0.56142553, False, 83, 10)] Confidence:0.27 Split Index:5, Information Gain: 1.2709, Class Label: 10.0\n",
      "Depth: 5, Rule: Antecedent:[(-0.16218547, True, 16, 4), (-0.69659581, True, 22, 10), (-0.48792292, True, 61, 10), (-0.3017266, False, 63, 9)]  ==> Consequent:[(-0.38072351, True, 76, 1)] Confidence:0.13 Split Index:4, Information Gain: 1.9219, Class Label: 10.0\n",
      "Depth: 2, Rule: Antecedent:[(-0.46938157, True, 74, 10)]  ==> Consequent:[(-0.56155893, False, 77, 10)] Confidence:0.03 Split Index:1, Information Gain: 0.9602, Class Label: 10.0\n",
      "Depth: 3, Rule: Antecedent:[(1.8767072, False, 0, 10), (1.9555487, True, 14, 10), (-0.22371666, True, 34, 10), (-0.17758956, True, 35, 10), (1.0011226, False, 38, 10), (0.76059167, False, 51, 10)]  ==> Consequent:[(-0.1957247, True, 57, 10)] Confidence:0.22 Split Index:6, Information Gain: 1.4588, Class Label: 10.0\n",
      "Depth: 4, Rule: Antecedent:[(-0.35979664, True, 19, 10), (-0.35290065, True, 41, 1), (-0.30501342, True, 50, 10), (-0.20951589, True, 51, 10), (-0.68594482, True, 57, 2), (-0.44800617, False, 68, 3)]  ==> Consequent:[(-0.76321621, True, 84, 2), (-0.48161825, False, 86, 10)] Confidence:0.3 Split Index:6, Information Gain: 1.4136, Class Label: 10.0\n",
      "Depth: 5, Rule: Antecedent:[(0.19109094, False, 7, 7), (0.09158915, True, 8, 4), (1.8400836, False, 10, 10), (0.92331538, False, 23, 9), (-0.55549588, True, 24, 7), (-0.31418908, True, 42, 10), (-0.078503984, False, 44, 10), (-0.60118642, True, 46, 10), (-0.40085772, True, 47, 8), (-0.44066297, True, 69, 10)]  ==> Consequent:[(-0.34377392, False, 72, 1)] Confidence:0.67 Split Index:10, Information Gain: 0.8315, Class Label: 10.0\n",
      "Depth: 6, Rule: Antecedent:[(-0.090606031, True, 27, 9), (-0.43636854, True, 54, 9), (-0.20899478, False, 59, 10)]  ==> Consequent:[(-0.39370396, False, 60, 1), (-0.7482304, True, 89, 10)] Confidence:0.4 Split Index:3, Information Gain: 0.7219, Class Label: 10.0\n",
      "Depth: 6, Rule: Antecedent:[(-0.24263786, True, 36, 10), (-0.55421972, True, 49, 10), (-0.270017, True, 52, 9), (-0.23839365, False, 59, 10)]  ==> Consequent:[(-0.53202238, True, 61, 9), (0.12860792, False, 80, 5)] Confidence:0.5 Split Index:4, Information Gain: 0.1909, Class Label: 10.0\n",
      "Depth: 7, Rule: Antecedent:[(1.0509903, True, 5, 7), (-0.10660426, True, 22, 10), (-0.46572632, True, 38, 9), (0.44644624, False, 49, 10)]  ==> Consequent:[(-0.49684741, True, 61, 9), (-0.31458452, False, 66, 1), (0.018109413, False, 71, 5)] Confidence:0.57 Split Index:4, Information Gain: 1.0000, Class Label: 1.0\n",
      "Depth: 8, Rule: Antecedent:[(-0.2482993, False, 43, 9)]  ==> Consequent:[(-0.20243104, True, 45, 10)] Confidence:0.04 Split Index:1, Information Gain: 1.0000, Class Label: 10.0\n",
      "\n",
      "Depth:  0\n",
      "Node class: 10\n",
      "Dataset Length: 381\n",
      "Rule Length: 7\n",
      "Information Gain: 0.2095\n",
      "True Branch:\n",
      "\n",
      "Depth:  1\n",
      "  Node class: 10.0\n",
      "  Dataset Length: 105\n",
      "  Rule Length: 4\n",
      "  Information Gain: 1.6290\n",
      "  True Branch:\n",
      "\n",
      "Depth:  2\n",
      "    Node class: 10.0\n",
      "    Dataset Length: 21\n",
      "    Rule Length: 10\n",
      "    Information Gain: 0.4445\n",
      "    True Branch:\n",
      "\n",
      "Depth:  3\n",
      "      Node class: 7.0\n",
      "      Dataset Length: 5\n",
      "      Rule Length: 5\n",
      "      Information Gain: 0.9852\n",
      "    False Branch:\n",
      "\n",
      "Depth:  3\n",
      "      Node class: 7.0\n",
      "      Dataset Length: 16\n",
      "      Rule Length: 7\n",
      "      Information Gain: 1.2495\n",
      "      True Branch:\n",
      "\n",
      "Depth:  4\n",
      "        Node class: 7.0\n",
      "        Dataset Length: 10\n",
      "        Rule Length: 4\n",
      "        Information Gain: 0.9864\n",
      "      False Branch:\n",
      "\n",
      "Depth:  4\n",
      "        Node class: 10.0\n",
      "        Dataset Length: 6\n",
      "        Rule Length: 6\n",
      "        Information Gain: 1.2709\n",
      "        True Branch:\n",
      "\n",
      "Depth:  5\n",
      "          Node class: 10.0\n",
      "          Dataset Length: 3\n",
      "          Rule Length: 5\n",
      "          Information Gain: 1.9219\n",
      "  False Branch:\n",
      "\n",
      "Depth:  2\n",
      "    Node class: 10.0\n",
      "    Dataset Length: 84\n",
      "    Rule Length: 2\n",
      "    Information Gain: 0.9602\n",
      "    False Branch:\n",
      "\n",
      "Depth:  3\n",
      "      Node class: 10.0\n",
      "      Dataset Length: 41\n",
      "      Rule Length: 7\n",
      "      Information Gain: 1.4588\n",
      "      True Branch:\n",
      "\n",
      "Depth:  4\n",
      "        Node class: 10.0\n",
      "        Dataset Length: 20\n",
      "        Rule Length: 8\n",
      "        Information Gain: 1.4136\n",
      "        True Branch:\n",
      "\n",
      "Depth:  5\n",
      "          Node class: 10.0\n",
      "          Dataset Length: 16\n",
      "          Rule Length: 11\n",
      "          Information Gain: 0.8315\n",
      "          True Branch:\n",
      "\n",
      "Depth:  6\n",
      "            Node class: 10.0\n",
      "            Dataset Length: 11\n",
      "            Rule Length: 5\n",
      "            Information Gain: 0.7219\n",
      "          False Branch:\n",
      "\n",
      "Depth:  6\n",
      "            Node class: 10.0\n",
      "            Dataset Length: 5\n",
      "            Rule Length: 6\n",
      "            Information Gain: 0.1909\n",
      "            False Branch:\n",
      "\n",
      "Depth:  7\n",
      "              Node class: 1.0\n",
      "              Dataset Length: 3\n",
      "              Rule Length: 7\n",
      "              Information Gain: 1.0000\n",
      "              False Branch:\n",
      "\n",
      "Depth:  8\n",
      "                Node class: 10.0\n",
      "                Dataset Length: 2\n",
      "                Rule Length: 2\n",
      "                Information Gain: 1.0000\n",
      "\n",
      "Tree Depth: 10\n",
      "Tree Size: 33\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "def visualize_tree(node, ts_list=[], graph=None, depth=0, parent_name=None, branch_label=None):\n",
    "    \n",
    "    # ts_list = []\n",
    "    if graph is None:\n",
    "        graph = Digraph()\n",
    "        root_label = f'Class: {node.class_label}\\nIG: {node.information_gain:.4f}' if node.information_gain is not None else f'Class: {node.class_label}\\nIG: None\\nDataset Length: {node.dataset_length}'\n",
    "        graph.node(name='root', label=root_label)\n",
    "        parent_name = 'root'\n",
    "\n",
    "    time_series = [i for i in range(node.dataset_length)]\n",
    "    current_name = f'node_{depth}_{id(node)}'\n",
    "    ig_label = f'{node.information_gain:.4f}' if node.information_gain is not None else 'None'\n",
    "    rule_label = f'{node.rule}' if node.rule else 'None'\n",
    "    label = f'Class: {node.class_label}\\nIG: {ig_label}\\nDataset Length: {node.dataset_length}'\n",
    "\n",
    "    # Only add node if information gain is not zero\n",
    "    if node.information_gain is not None and node.information_gain != 0:\n",
    "        graph.node(name=current_name, label=label)\n",
    "        if node.ts is not None:\n",
    "            ts_list.append((node.ts, branch_label))\n",
    "        if parent_name:\n",
    "            edge_label = branch_label if branch_label else ''\n",
    "            graph.edge(parent_name, current_name, label=edge_label)\n",
    "        if node.true_branch:\n",
    "            visualize_tree(node.true_branch, ts_list, graph, depth + 1, current_name, branch_label='True')\n",
    "        if node.false_branch:\n",
    "            visualize_tree(node.false_branch, ts_list, graph, depth + 1, current_name, branch_label='False')\n",
    "    \n",
    "    return graph, ts_list\n",
    "\n",
    "def tree_depth(node):\n",
    "    if node is None:\n",
    "        return 0\n",
    "    return 1 + max(tree_depth(node.true_branch), tree_depth(node.false_branch))\n",
    "\n",
    "def tree_size(node):\n",
    "    if node is None:\n",
    "        return 0\n",
    "    return 1 + tree_size(node.true_branch) + tree_size(node.false_branch)\n",
    "\n",
    "labels = dataset[0]\n",
    "\n",
    "MIN_SUPPORT = 0.05\n",
    "n_candidates = int(input(\"Enter the number of candidates: \"))\n",
    "tree = rpts_tree(dataset, labels, n_candidates, MIN_SUPPORT=MIN_SUPPORT, min_samples=1)\n",
    "print_tree(tree)\n",
    "tree_visualization, ts_list = visualize_tree(tree)\n",
    "tree_visualization.render('timeseries_output/decision_tree', format='png', cleanup=True)\n",
    "\n",
    "depth = tree_depth(tree)\n",
    "size = tree_size(tree)\n",
    "print()\n",
    "print(f\"Tree Depth: {depth}\")\n",
    "print(f\"Tree Size: {size}\")\n",
    "\n",
    "\n",
    "## UNCOMMENT TO SAVE TIME SERIES HTML FILES IT MIGHT TAKE A WHILE\n",
    "# for i, (ts, branch) in enumerate(ts_list):\n",
    "#     depth = i\n",
    "#     indexes = [j for j in range(len(ts))]\n",
    "    \n",
    "#     for t in ts:\n",
    "#         print_ts(ts, indexes, depth, branch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3b: SEQUENCE BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from math import log, exp\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "Z_dataset = []\n",
    "labels = []\n",
    "alphabet = set()\n",
    "\n",
    "with open(\"/Users/sebastianodarconso/Desktop/università/magistrale_progetti/bdss/exercise3/dataset_boosting/question.txt\") as f:\n",
    "    test_data = f.readlines()\n",
    "    for line in test_data:\n",
    "        (label, data) = line.split(\" \", 1)\n",
    "        data_array = [int(x) for x in data.split()]\n",
    "        Z_dataset.append(((1 if int(label[0]) == 1 else -1), data_array))\n",
    "\n",
    "for label, data in Z_dataset:\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "    for value in data:\n",
    "        alphabet.add(value)\n",
    "\n",
    "alphabet = sorted(list(alphabet))\n",
    "if len(labels) != 2:\n",
    "    raise ValueError(\"Only binary classification is supported\")\n",
    "\n",
    "count = Counter([label for label, _ in Z_dataset])\n",
    "print(\"Count of labels: \", count)   \n",
    "\n",
    "print(\"Z Dataset: \", Z_dataset)\n",
    "print(\"Len Z Dataset: \", len(Z_dataset))\n",
    "print(\"Labels: \", labels)\n",
    "print(\"Alphabet: \", alphabet)\n",
    "\n",
    "shuffled_Z = Z_dataset.copy()\n",
    "random.shuffle(shuffled_Z)\n",
    "\n",
    "print(\"Shuffled Z Dataset: \", shuffled_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_subsequence_(sequence, subsequence):\n",
    "    sub_len = len(subsequence)\n",
    "    for i in range(len(sequence) - sub_len + 1):\n",
    "        if sequence[i:i + sub_len] == subsequence:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def compute_error(dataset, labels, weights, subsequence, potential_label):\n",
    "    error = 0\n",
    "    for i, (seq, l) in enumerate(zip(dataset, labels)):\n",
    "        seq = seq[1]\n",
    "        contains_subsequence = contains_subsequence_(seq, subsequence)\n",
    "        if (contains_subsequence and l != potential_label) or (not contains_subsequence and l == potential_label):\n",
    "            error += weights[i]\n",
    "    return error\n",
    "\n",
    "def find_best_sequence(Z, weights):\n",
    "    best_sequence = None\n",
    "    best_sequence_label = None\n",
    "    best_error = float('inf')   \n",
    "    labels = [label for label, _ in Z]\n",
    "\n",
    "    for i in tqdm(range(len(Z))):\n",
    "        sequence = Z[i][1]\n",
    "        label = labels[i]\n",
    "\n",
    "        for j in range(len(sequence)):\n",
    "            for k in range(j + 1, len(sequence) + 1):\n",
    "                subsequence = sequence[j:k]\n",
    "                for potential_label in set(labels):\n",
    "                    error = compute_error(Z, labels, weights, subsequence, potential_label)\n",
    "                    if error < best_error:\n",
    "                        best_error = error\n",
    "                        best_sequence = subsequence\n",
    "                        best_sequence_label = potential_label\n",
    "    \n",
    "    return best_sequence, best_sequence_label, best_error\n",
    "\n",
    "def update_weights(dataset, labels, weights, best_sequence, best_label, alpha):\n",
    "    updated_weights = np.copy(weights)\n",
    "    for i, (seq, l) in enumerate(zip(dataset, labels)):\n",
    "        seq = seq[1]\n",
    "        contains_subsequence = contains_subsequence_(seq, best_sequence)\n",
    "\n",
    "        if(contains_subsequence and l == best_label) or (not contains_subsequence and l != best_label):\n",
    "            updated_weights[i] *= np.exp(alpha)\n",
    "        else:\n",
    "            updated_weights[i] *= np.exp(-alpha)\n",
    "    \n",
    "    return updated_weights / np.sum(updated_weights)\n",
    "\n",
    "\n",
    "def classify(sequence, rules, alphas):\n",
    "    score = 0\n",
    "    for alpha, (seq, label) in zip(alphas, rules):\n",
    "        if contains_subsequence_(sequence, seq):\n",
    "            score += alpha if label == 1 else -alpha\n",
    "        else:\n",
    "            score += -alpha if label == 1 else alpha\n",
    "\n",
    "    return np.sign(score)\n",
    "\n",
    "\n",
    "def compute_classifier_error(dataset, classifier, rules, alphas):\n",
    "    error_count = 0\n",
    "    for label, sequence in dataset:\n",
    "        if classifier(sequence, rules, alphas) != label:\n",
    "            pred = classifier(sequence)\n",
    "            print(f\"Classifier prediction: {pred}, true label: {label}\")\n",
    "            error_count += 1\n",
    "    return error_count / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def sequence_boosting(dataset, delta):\n",
    "\n",
    "    N = len(dataset)\n",
    "    weights = np.ones(N) / N\n",
    "    alphas = []\n",
    "    rules = []\n",
    "    labels = [label for label, _ in dataset]\n",
    "    classifier_errors = []\n",
    "    iteration_info = []\n",
    "    \n",
    "    t = 1\n",
    "\n",
    "    while True:\n",
    "\n",
    "        print(f\"Iteration: {t}\")\n",
    "        best_sequence, best_label, error = find_best_sequence(dataset, weights)\n",
    "\n",
    "        if best_sequence is None or error >= 0.5:\n",
    "            break \n",
    "\n",
    "        alpha_t = 0.5 * log((1 - error) / (error + 1e-10))\n",
    "\n",
    "        print(f\"Best sequence: {best_sequence}, best label: {best_label}, error: {error:.4f}, alpha: {alpha_t:.4f}\")\n",
    "        alphas.append(alpha_t)\n",
    "        rules.append((best_sequence, best_label))\n",
    "\n",
    "        weights = update_weights(dataset, labels, weights, best_sequence, best_label, alpha_t)\n",
    "        \n",
    "        error_count = 0\n",
    "        predictions = []\n",
    "        for label, sequence in dataset:\n",
    "            if classify(sequence, rules, alphas) != label:\n",
    "                pred = classify(sequence, rules, alphas)\n",
    "                predictions.append(pred)\n",
    "                error_count += 1\n",
    "\n",
    "        overall_classifier_error = error_count / len(dataset)\n",
    "        classifier_errors.append(overall_classifier_error)\n",
    "        print(f\"Classifier error: {overall_classifier_error}\")\n",
    "\n",
    "        iteration_info.append({\n",
    "            \"iteration\": t,\n",
    "            \"alpha\": alpha_t,\n",
    "            \"error\": error,\n",
    "            \"classigier_error\": overall_classifier_error,\n",
    "            \"weights\": weights.copy(),\n",
    "            \"best_sequence\": best_sequence,\n",
    "            \"best_label\": best_label,\n",
    "            \"predictions\": predictions\n",
    "        })\n",
    "\n",
    "        if overall_classifier_error < delta or overall_classifier_error == 0:\n",
    "            print(\"Classifier error below threshold.\") if overall_classifier_error < delta else print(\"Classifier error is zero.\")\n",
    "            break\n",
    "\n",
    "        if t > 100:\n",
    "            print(\"Reached maximum number of iterations.\")\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "    \n",
    "    final_classifier = lambda seq: classify(seq, rules, alphas)\n",
    "\n",
    "    final_predictions = [classify(seq, rules, alphas) for _, seq in dataset]\n",
    "    y_true = labels\n",
    "    y_pred = final_predictions\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return final_classifier, rules, alphas, classifier_errors, iteration_info, cm, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_shuffled_reduced = Counter([label for label, _ in shuffled_Z[:100]])\n",
    "print(\"Count of labels: \", count_shuffled_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier, rules, alphas, errors, iteration_info, cm, accuracy = sequence_boosting(shuffled_Z[:100], delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for label, sequence in shuffled_Z[:100]:\n",
    "    prediction = classifier(sequence)\n",
    "    if prediction == label:\n",
    "        count += 1\n",
    "    # print(f\"Predicted label = {prediction}, true label = {label}\")\n",
    "\n",
    "\n",
    "print(f\"Number of correct predictions: {count} out of {len(shuffled_Z[:100])}\")\n",
    "print(f\"Percentage of correct: {(count * 100) / len(shuffled_Z[:100]):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights interpretation\n",
    "Higher weights indicate data points that are difficult to classify for the previous ensembled classifier, this means that the new classifier needs to give more attention to those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_weights(iteration_info):\n",
    "    # Create a DataFrame to hold the data for all iterations\n",
    "    data = []\n",
    "    for info in iteration_info:\n",
    "        for idx, weight in enumerate(info['weights']):\n",
    "            data.append({'Data Point': idx, 'Weight': weight, 'Iteration': f\"Iteration {info['iteration']}\"})\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create the line plot using Plotly Express\n",
    "    fig = px.line(df, x='Data Point', y='Weight', color='Iteration',\n",
    "                  labels={'Data Point': 'Data Point', 'Weight': 'Weights', 'Iteration': 'Iterations'},\n",
    "                  title='Weights Distribution Over Iterations')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    fig.write_html(f\"boosting_output/weights.html\")\n",
    "\n",
    "plot_weights(iteration_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alphas interpretation\n",
    "\n",
    "- In the early iterations alpha values should be high as the model is learning and picking effective classifiers.\n",
    "- In mid iterations alpha values might decrease or fluctuate as the model begins to deal with harder to classify instances.\n",
    "- In late iterations alpha values might come to a plateau or become lower as each new classifier adds less incremental value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alphas(iteration_info):\n",
    "    # Extract iteration numbers and alpha values\n",
    "    data = [{'Iteration': info['iteration'], 'Alpha': info['alpha']} for info in iteration_info]\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create the line plot using Plotly Express\n",
    "    fig = px.line(df, x='Iteration', y='Alpha', \n",
    "                  labels={'Iteration': 'Iteration', 'Alpha': 'Alpha'},\n",
    "                  title='Alpha Values Over Iterations',\n",
    "                  markers=True) # Ensures markers are shown on the line\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    fig.write_html(f\"boosting_output/alphas.html\")\n",
    "\n",
    "\n",
    "plot_alphas(iteration_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier_errors(classifier_errors):\n",
    "    # Create a DataFrame with iterations and corresponding classifier errors\n",
    "    data = {\n",
    "        'Iteration': list(range(1, len(classifier_errors) + 1)),\n",
    "        'Classifier Error': classifier_errors\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create the line plot with markers using Plotly Express\n",
    "    fig = px.line(df, x='Iteration', y='Classifier Error', \n",
    "                  labels={'Iteration': 'Iteration', 'Classifier Error': 'Classifier Error'},\n",
    "                  title='Classifier Errors Over Iterations',\n",
    "                  markers=True) # Adds markers to the plot\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    fig.write_html(f\"boosting_output/error.html\")\n",
    "\n",
    "plot_classifier_errors(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "'''\n",
    "[[TRUE POSITIVE, FALSE NEGATIVE],\n",
    " [FALSE POSITIVE, TRUE NEGATIVE]]\n",
    "\n",
    "'''\n",
    "## This confusion matrix makes no sense since it's done on the training set, to make it work we should split the dataset into training and testing\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', cmap='Blues'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(\"boosting_output/confusion_matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(cm, class_names=['-1', '1]'], title='Confusion Matrix', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
